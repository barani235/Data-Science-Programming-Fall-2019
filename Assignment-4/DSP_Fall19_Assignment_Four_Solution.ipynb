{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Programming Assignment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Load the Banknote Authentication dataset and split it into a training set, a validation set and a test set</li>\n",
    "    <li>Create a Bagging classifier with Decision Tree classifiers</li>\n",
    "    <li>Then train various classifiers, such as Logistic Regression, Random Forest classifier, and an Extra-Trees classifier</li>\n",
    "    <li>Next, try to combine them into an ensemble that outperforms them all on the validation set, using a soft or hard voting classifier \n",
    "    <li>Once you have found one, try it on the test set</li>\n",
    "    <li>How much better does it perform compared to the individual classifiers?</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier,RandomForestClassifier,ExtraTreesClassifier,VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "banknotes=pd.read_csv(\"data_banknote_authentication.csv\",header='infer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the features and target data from the file read\n",
    "\n",
    "X=banknotes.iloc[:,:-1]\n",
    "y=banknotes.iloc[:,-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into train, validation and test sets\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a bagging classifier that trains an ensemble of 200 Decision Tree Classifiers, each trained on 100 training instances\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(random_state=42), n_estimators=200,\n",
    "    max_samples=100, bootstrap=True, n_jobs=-1, random_state=42)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of the Baging Classifier:  0.9381818181818182\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy score of the Baging Classifier: \",accuracy_score(y_test, bag_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression, Random Forest Classifier and Extra Trees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Logistic Regression, Random Forest Classifier and an Extra Trees Classifier\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "random_forest_clf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "extra_trees_clf = ExtraTreesClassifier(n_estimators=10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=42, solver='liblinear',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "Training the RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "            oob_score=False, random_state=42, verbose=0, warm_start=False)\n",
      "Training the ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "           oob_score=False, random_state=42, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "#Fit the above models on the train data\n",
    "\n",
    "estimators = [log_clf, random_forest_clf, extra_trees_clf]\n",
    "for estimator in estimators:\n",
    "    print(\"Training the\", estimator)\n",
    "    estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification scores of Logisitic Regression, Random Forest Classifier, and Extra Trees Classifier respectively are :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9772727272727273, 0.9863636363636363, 0.990909090909091]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Classification scores of Logisitic Regression, Random Forest Classifier, and Extra Trees Classifier respectively are :\")\n",
    "[estimator.score(X_val, y_val) for estimator in estimators]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensemble: Build a hard voting classifier by combining the models created above\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', random_forest_clf), ('et',extra_trees_clf)],\n",
    "    voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=42, solver='liblinear',\n",
       "          tol=0.0001, verbose=0, warm_start=False)), ('rf', Rando...imators=10, n_jobs=None,\n",
       "           oob_score=False, random_state=42, verbose=0, warm_start=False))],\n",
       "         flatten_transform=None, n_jobs=None, voting='hard', weights=None)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the ensemble\n",
    "\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification score of the hard voting classifier is  1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification score of the hard voting classifier is \",voting_clf.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification scores of Logisitic Regression, Random Forest Classifier, and Extra Trees Classifier on the test set respectively are :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9854545454545455, 0.9890909090909091, 0.9963636363636363]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Classification scores of Logisitic Regression, Random Forest Classifier, and Extra Trees Classifier on the test set respectively are :\")\n",
    "[estimator.score(X_test, y_test) for estimator in voting_clf.estimators_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The ensemble has a prediciting accuracy of 100% which is better than the best individual classifier, the Extra Trees Classifier which has a predicting accuracy of 99.63%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard voting classifier accuracy on the test set:  0.9963636363636363\n"
     ]
    }
   ],
   "source": [
    "#Now test the performance of the ensemble on the test data\n",
    "\n",
    "print(\"Hard voting classifier accuracy on the test set: \",voting_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf.voting = \"soft\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification scores of Random Forest Classifier, and Extra Trees Classifier on the test set respectively are :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9854545454545455, 0.9890909090909091, 0.9963636363636363]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Classification scores of Random Forest Classifier, and Extra Trees Classifier on the test set respectively are :\")\n",
    "[estimator.score(X_test, y_test) for estimator in voting_clf.estimators_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft voting classifier accuracy on the test set:  1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Soft voting classifier accuracy on the test set: \",voting_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Both the hard voting and the soft voting classifiers have performed better than the best of the individual classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
